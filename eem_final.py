# -*- coding: utf-8 -*-
"""EEM_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WP7GCYKZOo1hyhWYpgM9RMSRk7GRPvwd
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import scipy as sc
# %matplotlib inline
np.random.seed(1234)
from scipy.stats import bernoulli
import matplotlib.pyplot as plt
import math as m

"""
Generating data points:
X1 and X2 has n data points from distribution 1 & 2 respenctively;
Z1 is a vector of random 1 and 0 
generated from a bernoulli distribution with P(1)=p1 and P(0)=1-p1;
X is our final input vector having data point from both distributions
in proportion to their probabilitis.
"""
lambd1, lambd2 = 1, 3
p1 = 0.25
p2 = 1-p1
n = 200
X1 = np.random.exponential(1/lambd1,n)
X2 = np.random.exponential(1/lambd2,n)
z1 = bernoulli.rvs(p1,size=n)
z2 = 1-z1
X = z1*X1 + z2*X2
X = X.reshape([n,1])

"""
Plot of mixture
"""
x = np.linspace(0,4,100)
pdf1 = lambd1*np.exp(-lambd1*x)
pdf2 = lambd2*np.exp(-lambd2*x)
pdf = p1*pdf1+ p2*pdf2
plt.plot(x, pdf1, '-c', label = 'f$1$')
plt.plot(x, pdf2, '-r', label = 'f$2$')
plt.plot(x, pdf, '-k', label = 'Mixture : f')
plt.legend(loc="upper right")
plt.gca().set_ylabel('PDF')
plt.gca().set_xlabel('X')
plt.gca().set_title('PDF of Mixture')

def f(X, lambd):
  """
  Function to calculate the pdf of an exponential distribution
  """
  n = X.shape[0]
  f = lambd*np.exp(-lambd*X)
  f = f.reshape([n,1])
  return f

def responsibilities(X, p1, lambd1, lambd2):
  """
  Expectation Step + Likelihood:
  Calculating posterior probabilities(Responsibilities)
  and log likelihood.
  """
  p2 = 1-p1
  R1 = f(X,lambd1)*p1
  R2 = f(X,lambd2)*p2
  Likelihood = R1 + R2
  R1 = R1/Likelihood
  R2 = R2/Likelihood
  R = np.concatenate((R1,R2),axis=1)
  Log_ll = np.log(Likelihood).sum()

  return R, Log_ll

"""
Maximisation Step:
Updating the estimates in each successive iterations.
t : avariable to indicate each successive iteratin
tol : tolerance to determine the difference in each iteration
LLL_values and Likelihood store the values of Loglikelihood and likelihood.
"""

LLL_temp = []
Likelihood = []
t = 0
tol = 1e-3
R,Log_ll_temp = responsibilities(X, p1, lambd1, lambd2)
LLL_temp.append(3*Log_ll_temp)
LLL_temp.append(2*Log_ll_temp)
while(LLL_temp[t+1]-LLL_temp[t]>tol):
  R, Log_ll = responsibilities(X, p1, lambd1, lambd2)
  p = np.zeros([1,2])
  p = R.sum(0) / n
  p1 = p[0]
  lambd1, lambd2 = (p*n)/(R*X).sum(0)
  LLL_temp.append(Log_ll)
  Likelihood.append(10**(Log_ll))
  t += 1  

print('Iterations to converge: %d | log-likelihood: %f'%(t, Log_ll))
LLL_values = LLL_temp[2:]
print('\nTheta:\t Pi_1: %f|Pi_2 = %f|Lambda_1:%f|Lambda_2: %f'%(p[0],p[1],lambd1,lambd2))

"""
Plot of Log Likelihood
"""
plt.plot(LLL_values)
plt.gca().set_ylabel('Log Likelihood')
plt.gca().set_xlabel('Iteration $t$')
plt.gca().set_title('Iteration vs Log Likelihood')

"""
Plot of Log Likelihood
"""
plt.plot(Likelihood)
plt.gca().set_ylabel('Likelihood')
plt.gca().set_xlabel('Iteration $t$')
plt.gca().set_title('Iteration vs Likelihood')